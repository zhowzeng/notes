# UQ360

<https://github.com/IBM/UQ360>

## Algorithms & Metrics

![](https://i.imgur.com/ZiEYBP7.png)

- Supported Uncertainty Evaluation Metrics (衡量 UQ 的表現，不是模型表現)
  - clf
    - Expected Calibration Error (ECE)
    - Brier Score
  - reg
    - Prediction Interval Coverage Probability (PICP)
    - Mean Prediction Interval Width (MPIW)
  - operation-point agnostic
    - Uncertainty Characteristic Curve (UCC)
- Supported Uncertainty Estimation Algorithms
  - intrinsic: inherently provides an uncertainty estimate along with its predictions
    - variational Bayesian neural networks (BNNs)
    - Gaussian processes
    - quantile regression
    - hetero/homo-scedastic neuralnetworks
    - Horseshoe BNNs
    - Infinitesimal Jackknife (IJ) based
  - extrinsic: extract uncertainties post-hoc
    - meta-models
    - improve the uncertainty estimation quality
      - isotonic regression
      - Platt-scaling
      - auxiliary interval predictors
      - UCC-Recalibration
- Choosing UQ Algorithms and Metrics

  ![](https://i.imgur.com/WQmN9xS.png)

- A good UQ method should generate uncertainty estimates that work reliably. If so, we say the uncertainty estimates are well-calibrated.
  - for regression, the actual outcome should fall within the prediction interval
  - for classification, it should align with the predicted class with a probability close to the stated confidence
- A well-calibrated but large prediction interval (for regression) or low-confidence prediction (for classification) may be useless in practice. When assessing the quality of UQ estimation, it is often necessary to consider both calibration and level of confidence.
- Like a model’s prediction performance, the quality of UQ should also be evaluated with a set of **test** data.
- To understand where the uncertainty comes from for a model and how to improve it, it is helpful to examine **data uncertainty** and **model uncertainty** separately. 重點是，靠北，怎麼分開 uncertainty?

  ![](https://i.imgur.com/V9KEGkh.png)

  - One way to reduce data uncertainty is by adding new features that could account for the fluctuation.
  - model uncertainty can be mitigated by collecting more training data. Examining the detailed model UQ information by different feature-values could guide the data collection choices
